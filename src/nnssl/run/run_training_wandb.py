import os
import signal
import socket

from typing import get_args
from datetime import timedelta
from typing import Type, Union, Optional

import wandb
import torch.cuda
import torch.distributed as dist
import torch.multiprocessing as mp

from loguru import logger
from batchgenerators.utilities.file_and_folder_operations import join, isfile, load_json

import nnssl
from nnssl.experiment_planning.experiment_planners.plan_wandb import Plan_wandb
from nnssl.experiment_planning.experiment_planners.plan import PREPROCESS_SPACING_STYLES
from nnssl.paths import nnssl_preprocessed
from nnssl.run.load_pretrained_weights import load_pretrained_weights
from nnssl.training.nnsslTrainer.AbstractTrainer import AbstractBaseTrainer
from nnssl.utilities.dataset_name_id_conversion import maybe_convert_to_dataset_name
from nnssl.utilities.find_class_by_name import recursive_find_python_class
from torch.backends import cudnn


def find_free_network_port() -> int:
    """Finds a free port on localhost.

    It is useful in single-node training when we don't want to connect to a real main node but have to set the
    `MASTER_PORT` environment variable.
    """
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.bind(("", 0))
    port = s.getsockname()[1]
    s.close()
    return port


def get_trainer_from_args(
    dataset_name_or_id: Union[int, str],
    configuration: str,
    fold: int,
    trainer_name: str = "nnsslTrainer",
    plans_identifier: str = "nnsslPlans",
    device: torch.device = torch.device("cuda"),
    # *args,
    **kwargs,
):
    # load nnunet class and do sanity checks
    nnssl_trainer_cls: Type[AbstractBaseTrainer] = recursive_find_python_class(
        join(nnssl.__path__[0], "training", "nnsslTrainer"),
        trainer_name,
        "nnssl.training.nnsslTrainer",
    )
    if nnssl_trainer_cls is None:
        raise RuntimeError(
            f"Could not find requested nnunet trainer {trainer_name} in "
            f"nnssl.training.nnsslTrainer ("
            f'{join(nnssl.__path__[0], "training", "nnsslTrainer")}). If it is located somewhere '
            f"else, please move it there."
        )
    assert issubclass(nnssl_trainer_cls, AbstractBaseTrainer), (
        "The requested nnunet trainer class must inherit from " "nnsslTrainer"
    )

    # handle dataset input. If it's an ID we need to convert to int from string
    if dataset_name_or_id.startswith("Dataset"):
        pass
    else:
        try:
            dataset_name_or_id = int(dataset_name_or_id)
        except ValueError:
            raise ValueError(
                f"dataset_name_or_id must either be an integer or a valid dataset name with the pattern "
                f"DatasetXXX_YYY where XXX are the three(!) task ID digits. Your "
                f"input: {dataset_name_or_id}"
            )

    # initialize nnunet trainer
    preprocessed_dataset_folder_base = join(
        nnssl_preprocessed, maybe_convert_to_dataset_name(dataset_name_or_id)
    )
    plans_file = join(preprocessed_dataset_folder_base, plans_identifier + ".json")
    plans: Plan_wandb = Plan_wandb.load_from_file(plans_file)
    pretrain_json = load_json(
        join(preprocessed_dataset_folder_base, f"pretrain_data__{configuration}.json")
    )
    nnssl_trainer: AbstractBaseTrainer = nnssl_trainer_cls(
        plans,
        configuration,
        fold,
        pretrain_json,
        device,
        # *args,
        # **kwargs
    )
    return nnssl_trainer


def maybe_load_checkpoint(
    nnunet_trainer: AbstractBaseTrainer,
    continue_training: bool,
    validation_only: bool,
    pretrained_weights_file: str = None,
):
    if continue_training and pretrained_weights_file is not None:
        raise RuntimeError(
            "Cannot both continue a training AND load pretrained weights. Pretrained weights can only "
            "be used at the beginning of the training."
        )

    if continue_training:
        logger.info("Attempting to continue training...")
        expected_checkpoint_file = join(
            nnunet_trainer.output_folder, "checkpoint_final.pth"
        )
        if not isfile(expected_checkpoint_file):
            expected_checkpoint_file = join(
                nnunet_trainer.output_folder, "checkpoint_latest.pth"
            )
        # special case where --c is used to run a previously aborted validation
        if not isfile(expected_checkpoint_file):
            expected_checkpoint_file = join(
                nnunet_trainer.output_folder, "checkpoint_best.pth"
            )
        # if not isfile(expected_checkpoint_file):
        #     print(
        #         f"WARNING: Cannot continue training because there seems to be no checkpoint available to "
        #         f"continue from. Starting a new training..."
        #     )
        # raise RuntimeError(
        #     f"Cannot continue training because there seems to be no checkpoint available to continue from. Starting a new training..."
        # )
        if isfile(expected_checkpoint_file):
            logger.info(
                f"Using {expected_checkpoint_file} as the starting checkpoint for training..."
            )
        else:
            expected_checkpoint_file = None
            logger.info(f"No starting checkpoint available, starting a new training...")
    elif validation_only:
        expected_checkpoint_file = join(
            nnunet_trainer.output_folder, "checkpoint_final.pth"
        )
        if not isfile(expected_checkpoint_file):
            raise RuntimeError(
                f"Cannot run validation because the training is not finished yet!"
            )
    else:
        if pretrained_weights_file is not None:
            if not nnunet_trainer.was_initialized:
                nnunet_trainer.initialize()
            load_pretrained_weights(
                nnunet_trainer.network, pretrained_weights_file, verbose=True
            )
        expected_checkpoint_file = None

    if expected_checkpoint_file is not None:
        try:
            nnunet_trainer.load_checkpoint(expected_checkpoint_file)
        except EOFError:
            os.remove(expected_checkpoint_file)


def setup_ddp(rank, world_size):
    # initialize the process group
    # Unpacking actually takes about
    dist.init_process_group(
        "nccl", rank=rank, world_size=world_size, timeout=timedelta(minutes=25)
    )


def cleanup_ddp():
    dist.destroy_process_group()


def run_ddp(
    rank,
    dataset_name_or_id,
    configuration,
    fold,
    tr,
    p,
    disable_checkpointing,
    c,
    val,
    pretrained_weights,
    npz,
    val_with_best,
    world_size,
    add_params,
    use_wandb: bool = False,
):
    setup_ddp(rank, world_size)
    torch.cuda.set_device(torch.device("cuda", dist.get_rank()))
    device = torch.device(f"cuda:{rank}")
    nnunet_trainer = get_trainer_from_args(
        dataset_name_or_id, configuration, fold, tr, p, device, **add_params
    )

    if disable_checkpointing:
        nnunet_trainer.disable_checkpointing = disable_checkpointing

    assert not (c and val), f"Cannot set --c and --val flag at the same time. Dummy."

    maybe_load_checkpoint(nnunet_trainer, c, val, pretrained_weights)

    if torch.cuda.is_available():
        cudnn.deterministic = False
        cudnn.benchmark = True

    # Prepare the auto-exiting in case wall-time is exceeded.
    #  This sets a internal flag, letting the trainer know it's 10 minutes till wall-clock time is up.
    signal.signal(signal.SIGUSR1, nnunet_trainer.exit_training)

    if not val:
        nnunet_trainer.run_training(use_wandb)

    if val_with_best:
        nnunet_trainer.load_checkpoint(
            join(nnunet_trainer.output_folder, "checkpoint_best.pth")
        )
    nnunet_trainer.perform_actual_validation(npz)
    cleanup_ddp()


def run_training(
    dataset_name_or_id: Union[str, int],
    configuration: str,
    fold: Union[int, str],
    trainer_class_name: str = "nnsslTrainer",
    plans_identifier: str = "nnsslPlans",
    pretrained_weights: Optional[str] = None,
    num_gpus: int = 1,
    export_validation_probabilities: bool = False,
    continue_training: bool = False,
    only_run_validation: bool = False,
    disable_checkpointing: bool = False,
    val_with_best: bool = False,
    device: torch.device = torch.device("cuda"),
    *args,
    **kwargs,
):
    if isinstance(fold, str):
        if fold != "all":
            try:
                fold = int(fold)
            except ValueError as e:
                print(
                    f'Unable to convert given value for fold to int: {fold}. fold must bei either "all" or an integer!'
                )
                raise e

    if val_with_best:
        assert (
            not disable_checkpointing
        ), "--val_best is not compatible with --disable_checkpointing"

    try:
        entity = os.environ.get("WANDB_ENTITY", None)
        project = os.environ.get("WANDB_PROJECT", "nnssl")
        run_id = os.environ.get("WANDB_RUN_ID", None)

        wandb.init(
            entity=entity,
            project=project,
            id=run_id,
            name=f"{dataset_name_or_id}_{configuration}_fold{fold}_{trainer_class_name}_{plans_identifier}",
        )
    except wandb.Error as e:
        print(
            "Failed to initialize wandb. "
            "Make sure you have set the WANDB_ENTITY and WANDB_PROJECT environment variables correctly."
        )
        raise e

    if num_gpus > 1:
        assert (
            device.type == "cuda"
        ), f"DDP training (triggered by num_gpus > 1) is only implemented for cuda devices. Your device: {device}"

        os.environ["MASTER_ADDR"] = "localhost"
        if "MASTER_PORT" not in os.environ.keys():
            port = str(find_free_network_port())
            print(f"using port {port}")
            os.environ["MASTER_PORT"] = port  # str(port)
        add_params = kwargs

        mp.spawn(
            run_ddp,
            args=(
                dataset_name_or_id,
                configuration,
                fold,
                trainer_class_name,
                plans_identifier,
                disable_checkpointing,
                continue_training,
                only_run_validation,
                pretrained_weights,
                export_validation_probabilities,
                val_with_best,
                num_gpus,
                add_params,
                wandb.run is not None,  # use_wandb
            ),
            nprocs=num_gpus,
            join=True,
        )
    else:
        nnunet_trainer = get_trainer_from_args(
            dataset_name_or_id,
            configuration,
            fold,
            trainer_class_name,
            plans_identifier,
            device=device,
            **kwargs,
        )

        # Prepare the auto-exiting in case wall-time is exceeded.
        #  This sets a internal flag, letting the trainer know it's 10 minutes till wall-clock time is up.
        signal.signal(signal.SIGUSR1, nnunet_trainer.exit_training)

        if disable_checkpointing:
            nnunet_trainer.disable_checkpointing = disable_checkpointing

        assert not (
            continue_training and only_run_validation
        ), f"Cannot set --c and --val flag at the same time. Dummy."

        maybe_load_checkpoint(
            nnunet_trainer, continue_training, only_run_validation, pretrained_weights
        )

        if torch.cuda.is_available():
            cudnn.deterministic = False
            cudnn.benchmark = True

        if not only_run_validation:
            nnunet_trainer.run_training(using_wandb=True)

        if val_with_best:
            nnunet_trainer.load_checkpoint(
                join(nnunet_trainer.output_folder, "checkpoint_best.pth")
            )

        nnunet_trainer.perform_actual_validation(export_validation_probabilities)

    if wandb.run is not None:
        wandb.finish()


def run_training_entry():
    import argparse

    parser = argparse.ArgumentParser()

    parser.add_argument(
        "dataset_name_or_id", type=str, help="Dataset name or ID to train with"
    )
    parser.add_argument(
        "configuration",
        type=str,
        help="Configuration that should be trained",
        choices=get_args(PREPROCESS_SPACING_STYLES),
    )
    parser.add_argument(
        "-tr",
        type=str,
        required=False,
        default="nnsslTrainer",
        help="[OPTIONAL] Use this flag to specify a custom trainer. Default: nnsslTrainer",
    )
    parser.add_argument(
        "-p",
        type=str,
        required=False,
        default="nnsslPlans",
        help="[OPTIONAL] Use this flag to specify a custom plans identifier. Default: nnSSLPlans",
    )
    parser.add_argument(
        "-fold",
        type=str,
        required=False,
        default="all",
        help="[OPTIONAL] Use this flag to specify the fold to train on. Default: all. "
        "If you want to train on a specific fold, use an integer (e.g. 0-5). "
        "If you want to train on all folds, use 'all'.",
    )
    parser.add_argument(
        "-pretrained_weights",
        type=str,
        required=False,
        default=None,
        help="[OPTIONAL] path to nnU-Net checkpoint file to be used as pretrained model. Will only "
        "be used when actually training. Beta. Use with caution.",
    )
    parser.add_argument(
        "-num_gpus",
        type=int,
        default=1,
        required=False,
        help="Specify the number of GPUs to use for training",
    )
    parser.add_argument(
        "--npz",
        action="store_true",
        required=False,
        help="[OPTIONAL] Save softmax predictions from final validation as npz files (in addition to predicted "
        "segmentations). Needed for finding the best ensemble.",
    )
    parser.add_argument(
        "--c",
        action="store_true",
        required=False,
        help="[OPTIONAL] Continue training from latest checkpoint",
    )
    parser.add_argument(
        "--val",
        action="store_true",
        required=False,
        help="[OPTIONAL] Set this flag to only run the validation. Requires training to have finished.",
    )
    parser.add_argument(
        "--val_best",
        action="store_true",
        required=False,
        help="[OPTIONAL] If set, the validation will be performed with the checkpoint_best instead "
        "of checkpoint_final. NOT COMPATIBLE with --disable_checkpointing! "
        "WARNING: This will use the same 'validation' folder as the regular validation "
        "with no way of distinguishing the two!",
    )
    parser.add_argument(
        "--disable_checkpointing",
        action="store_true",
        required=False,
        help="[OPTIONAL] Set this flag to disable checkpointing. Ideal for testing things out and "
        "you dont want to flood your hard drive with checkpoints.",
    )
    parser.add_argument(
        "-device",
        type=str,
        default="cuda",
        required=False,
        help="Use this to set the device the training should run with. Available options are 'cuda' "
        "(GPU), 'cpu' (CPU) and 'mps' (Apple M1/M2). Do NOT use this to set which GPU ID! "
        "Use CUDA_VISIBLE_DEVICES=X nnUNetv2_train [...] instead!",
    )
    args = parser.parse_args()

    assert args.device in [
        "cpu",
        "cuda",
        "mps",
    ], f"-device must be either cpu, mps or cuda. Other devices are not tested/supported. Got: {args.device}."

    # ------------------------------- Post Parsers ------------------------------- #

    dataset_name = args.dataset_name_or_id
    config = args.configuration

    assert (
        os.environ.get("nnssl_results") is not None
    ), "nnssl_results not set. Stopping as no outputs would be written otherwise."

    if args.device == "cpu":
        # let's allow torch to use hella threads
        import multiprocessing

        torch.set_num_threads(multiprocessing.cpu_count())
        device = torch.device("cpu")
    elif args.device == "cuda":
        # multithreading in torch doesn't help nnU-Net if run on GPU
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)
        device = torch.device("cuda")
    else:
        device = torch.device("mps")
    run_training(
        dataset_name,
        config,
        args.fold,
        args.tr,
        args.p,
        args.pretrained_weights,
        args.num_gpus,
        args.npz,
        args.c,
        args.val,
        args.disable_checkpointing,
        args.val_best,
        device,
    )


if __name__ == "__main__":
    os.environ["WANDB__SERVICE_WAIT"] = "500"
    run_training_entry()
